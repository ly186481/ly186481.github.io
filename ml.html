<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Li's Data Analysis Workshop</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
							<header id="header">
								<a href="index.html" class="logo"><strong>Li's</strong> Data Analysis Workshop</a>
								<ul class="icons">
									<li><a href="https://github.com/ly186481" class="icon brands fa-github"><span class="label">github</span></a></li>
								</ul>
							</header>

							<!-- Content -->
								<section>
									<header class="main">
										<h2>Machine Learning</h2>
									</header>
									<p>Machine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy</p>
									
									<hr class="major" />
								
									<h2>Lazy Learning-Classification Using Nearest Neighbors</h2>
									<p>Nearest Neighbors requires a distance function, or a formula that measures the similarity between two distances.
										kNN algorithm uses Euclidean distance, which is the distance one would measure if you could use a ruler to connect two points.
								
									</p>
									<p>k=1, 1NN, The one nearest neighbors type will classification-type the type of the subject. 1NN.
									</p>
									<p>k=3, 3NN, it performs a vote among the three nearest neighbors. The majority class among these neighbors will classify the subject. 
									</p>
									<p>Choosing an appropriate k.
										The balance between overfitting and underfitting the training data is a problem known as the bias-variance tradeoff.
										Choosing a large k reduces the impact or variance caused by noisy data, but can bias the learner such that it runs the risk of ignoring small, but important patterns.
										On the opposite, using single small k allow noisy data or outliers, to unduly influence the classification.
									</p>
									<p>k is set somewhere between 3-10. Commonly used to set k equal to the square root of the number of training examples.
										Other way is to approach test several k values on a variety of test datasets and choose the one that delivers the best classification performance.
									</p>
									<p><b>Diagnosing breast cancer with the kNN algorithm</b>.</p>
									<p>Detecting cancer by applying the kNN algorithm to measurements of biopsied cells from women with abnormal breast masses.
										<ul>
											<li>Step 1 - Collecting Data,
												<a href="https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic">"Breast Cancer Wisconsin Diagnostic""</a>.
											</li>
											<li>Step 2 - Exploring and preparing the data,</li>
											<pre><code>
<b>#sample, Lazy learning kNN</b>
wbcd <- read.csv("wisc_bc_data.csv",stringsAsFactors = FALSE)

#exclude ID column
wbcd <- wbcd[-1]

#browse the structure of diagnosis column
table(wbcd$diagnosis)

#label B,M
wbcd$diagnosis <- factor(wbcd$diagnosis, levels=c("B","M"),
                         labels=c("Benign","Malignant"))

round(prop.table(table(wbcd$diagnosis))*100,digits=1)

summary(wbcd[c("radius_mean","area_mean","smoothness_mean")])

#smoothness and radius has big gap in distance, so it need normalization
normalize <- function(x){
  return((x-min(x))/(max(x)-min(x)))
  }

#test normalize function
normalize(c(1,2,3,4,5))
normalize(c(10,20,30,40,50))

wbcd_n <- as.data.frame(lapply(wbcd[2:31],normalize))
summary(wbcd_n$area_mean)

wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]

#training kNN model, store these class labels in factor vectors
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]

</code></pre>

											<li>Step 3 - training a model on the data</li>
<pre><code>
install.packages("class")
library(class)

install.packages("gmodels")
library(gmodels)

wbcd_test_pred <- knn(train = wbcd_train,
                      test = wbcd_test,
                      cl = wbcd_train_labels,
                      k = 21)

		</code></pre>	
		
											<li>Step 4 - evaluating model performance</li>
<pre><code>
CrossTable(x=wbcd_test_labels,
		y=wbcd_test_pred,
		prop.chisq = FALSE)

</code></pre>

<li>Step 5 - Improving model performance</li>
<li> To improve performance, approaching althernaitive method for rescaling numeric features and trying several different values for k.
	Applying z-score normalization.
</li>

<pre><code>
#rescaling with z-score standardization, mean should be always 0.
wbcd_z <- as.data.frame(scale(wbcd[-1]))
summary(wbcd_z$area_mean)
	
wbcd_train <- wbcd_z[1:469, ]
wbcd_test <- wbcd_z[470:569, ]
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
wbcd_test_pred <- knn(train=wbcd_train,test=wbcd_test,cl=wbcd_train_labels, k=21)
CrossTable(x=wbcd_test_labels,y=wbcd_test_pred, prop.chisq = FALSE)

</code></pre>

<pre><code>
#try several different values of k
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
	
k_values <- c(1, 5, 11, 15, 21, 27)
	
for(k_val in k_values){
wbcd_test_pred <- knn(train = wbcd_train,
						test = wbcd_test, 
						cl=wbcd_train_labels,
						k=k_val)

#To see the result of my knn 
CrossTable (x = wbcd_test_labels, 
			y=wbcd_test_pred, 
			prop.chiaq=FALSE)
}
	

</code></pre>
												
											

										</ul>


									</p>
									
									<hr class="major" />

									<h2>Rescaling Features</h2>
									<p><b>Min-Max Normalization</b>. This process transforms a feature such that all of its values fall in a range between 0 and 1.</p>

									<p><b>z-score standardization</b>. It subtracts the mean value of feature X and divides by the standard deviation of X.</p>

									<hr class="major" />

									<p><b>Case Study2-The Abalone Problem Statement(python)</b>.</p>
									<p>Find the age measurements of a large number of abalone along with a lot of other physical measurements. The goal of the project is to develop a model that can predict the age of an abalone based purely on the other physical measurement.
										<ul>
											<li>Step 1 - Collecting Data
												<pre><code>
import pandas as pd
url = ( "https://archive.ics.uci.edu/ml/machine-learning-databases" "/abalone/abalone.data")
abalone = pd.read_csv(url, header=None)
												</code>
											</li>
											<li>Step 2 - Exploring and preparing the data,</li>
											<pre><code>
<b>#Checking that imported data correctly</b>
abalone.head()

<b>#data does not column names. So check on UCI machine learning repositiry for matching names</b>

abalone.columns = [
	"Sex",
	"Length",
	"Diameter",
	"Height",
	"Whole weight",
	"Sucked weight",
	"Viscera weight",
	"Shell weight"
	"Rings",
	]

	abalone.head()
<b>Now you can check the columns have their names.
	Delete the "Sex" column because we do not need this for our test.
</b>
abalone = abalone.drop("Sex",axix=1)

<b>Before proceeding to ml method. Browse the data by exploratory statistics and graphs.
	Target variable is Rings. So lets plot the data with pandas plotting function.
</b>
import matplotlib.pyplot as plot
abalone["Rings"].hist(bins=15)
plt.show()

<b>Next is correlation exploration with variables.
	A strong correlation between an independent variable and my goal variable is a good sign,
	as this would confirm that physical measurements and age are related.
</b>

correlation_matrix = abalone.corr()
correlation_matrix["Rings"]

Length            0.556720
Diameter          0.574660
Height            0.557467
Whole weight      0.540390
Shucked weight    0.420884
Viscera weight    0.503819
Shell weight      0.627574
Rings             1.000000

<b>The correlation is close they are to 1, it is good sign.
	However, the variables seem to have not that significant correlation with the Rings.
	So I need to carefully observe the test result using kNN algorithm.
</b>

</code></pre>



					<li>Step 3 - training a model on the data</li>
<pre><code>
<b>This test is also using Euclidean distance calculation.
	For normalization, we will use numpy linalg.norm().
	For example,
</b>

import numpy as np 
a = np.array([2,2])
b = np.array([4,4])
np.linalg.norm(a - b)
2.8284271247461903

<b> To proceed with abalone dataset</b>

X = abalone.drop("Rings", axis=1)
X = X.values
y = abalone["Rings"]
y = y.values

<b>X is independent varibales and y is the dependent variables of my model.</b>
		</code></pre>	
		
											<li>Step 4 - evaluating model performance</li>
<pre><code>
CrossTable(x=wbcd_test_labels,
		y=wbcd_test_pred,
		prop.chisq = FALSE)

</code></pre>

<li>Step 5 - Improving model performance</li>
<li> To improve performance, approaching althernaitive method for rescaling numeric features and trying several different values for k.
	Applying z-score normalization.
</li>

<pre><code>
#rescaling with z-score standardization, mean should be always 0.
wbcd_z <- as.data.frame(scale(wbcd[-1]))
summary(wbcd_z$area_mean)
	
wbcd_train <- wbcd_z[1:469, ]
wbcd_test <- wbcd_z[470:569, ]
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
wbcd_test_pred <- knn(train=wbcd_train,test=wbcd_test,cl=wbcd_train_labels, k=21)
CrossTable(x=wbcd_test_labels,y=wbcd_test_pred, prop.chisq = FALSE)

</code></pre>

<pre><code>
#try several different values of k
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
	
k_values <- c(1, 5, 11, 15, 21, 27)
	
for(k_val in k_values){
wbcd_test_pred <- knn(train = wbcd_train,
						test = wbcd_test, 
						cl=wbcd_train_labels,
						k=k_val)

#To see the result of my knn 
CrossTable (x = wbcd_test_labels, 
			y=wbcd_test_pred, 
			prop.chiaq=FALSE)
}
	

</code></pre>
												
											

										</ul>
								</section>
					
					
					
					<ul class="pagination">
						
						<li><a href="ml.html" class="page active">1</a></li>
						<li><a href="ml2.html" class="page">2</a></li>
						<li><a href="ml3.html" class="page">3</a></li>
						
						
					</ul>
						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Search -->
								<section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
									</form>
								</section>

							<!-- Menu -->
							<nav id="menu">
								<header class="major">
									<h2>Menu</h2>
								</header>
								<ul>
									<li><a href="index.html">Homepage</a></li>
									<li><a href="casestudy.html">Case Study</a></li>
									<li><a href="dataanalysis.html">Data Analysis</a></li>
									<li>
										<span class="opener">Method</span>
										<ul>
											<li><a href="ml.html">Machine Learning</a></li>
											<li><a href="bigdata.html">Big Data</a></li>
										</ul>
									
									</li>
								</ul>
							</nav>

							

							

							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; Copyright 2025 by Li. All rights reserved.</p>
								</footer>

						</div>
					</div>


			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>